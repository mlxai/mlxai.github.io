<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <!-- (1) Optimize for mobile versions: http://goo.gl/EOpFl -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- (1) force latest IE rendering engine: bit.ly/1c8EiC9 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>A Modular Approach to Implementing Fully-Connected Neural Networks</title>
  <meta name="description" content="I write this post to clarify non-trivial issues in implementing forward and backward layers of fully-connected neural networks. The code is short and seems intuitive. However, I would like to elabo..." />

  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@nimitpattanasri" />
    <meta name="twitter:title" content="A Modular Approach to Implementing Fully-Connected Neural Networks" />
    <meta name="twitter:image" content="https://mlxai.github.io/assets/images/logo.jpg" />
    
    <meta name="twitter:description"  content="I write this post to clarify non-trivial issues in implementing forward and backward layers of fully-connected neural networks. The code is short and seems intuitive. However, I would like to elabo..." />
    
  
  
  <meta property="og:site_name" content="ML x AI" />
  <meta property="og:title" content="A Modular Approach to Implementing Fully-Connected Neural Networks"/>
  
  <meta property="og:description" content="I write this post to clarify non-trivial issues in implementing forward and backward layers of fully-connected neural networks. The code is short and seems intuitive. However, I would like to elabo..." />
  
  <meta property="og:image" content="https://mlxai.github.io/assets/images/logo.jpg" />
  <meta property="og:url" content="https://mlxai.github.io/2017/01/10/a-modular-approach-to-implementing-fully-connected-neural-networks.html" >
  <meta property="og:type" content="blog" />
  <meta property="article:published_time" content="2017-01-10T00:00:00+07:00">

  <link rel="canonical" href="https://mlxai.github.io/2017/01/10/a-modular-approach-to-implementing-fully-connected-neural-networks.html"/>
  <link rel="shortcut icon" href="/assets/images/favicon.png" type="image/png"/>
  <link rel="stylesheet" href="//brick.a.ssl.fastly.net/Linux+Libertine:400,400i,700,700i/Open+Sans:400,400i,700,700i">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="print" href="/css/print.css" />
</head>

  <body itemscope itemtype="http://schema.org/Article">
    <!-- header start -->


<!-- header end -->

    <main class="content" role="main">
      <article class="post">
        
        <div class="noarticleimage">
          <div class="post-meta">
            <h1 class="post-title">A Modular Approach to Implementing Fully-Connected Neural Networks</h1>
            <div class="cf post-meta-text">
              <div class="author-image" style="background-image: url(/assets/images/nimit-opt.jpg)">Blog Logo</div>
              <h4 class="author-name" itemprop="author" itemscope itemtype="http://schema.org/Person"></h4>
              on
              <time datetime="2017-01-10T00:00:00+07:00">10 Jan 2017</time>
              <!-- , tagged on <span class="post-tag-">, <a href="/tag/"></a></span> -->
            </div>
          </div>
        </div>
        <br>
        <br>
        <br>
        
        <section class="post-content">
          <div class="post-reading">
            <span class="post-reading-time"></span> read
          </div>
          <a name="topofpage"></a>
          <p>I write this post to clarify non-trivial issues in implementing forward and backward layers of fully-connected neural networks. The code is short and seems intuitive. However, I would like to elaborate on finding partial derivative w.r.t. the bias, that is, clarifying the expression <code>db = np.sum(dout, axis=0)</code> for the uninitiated.</p>

<p>To become a backpropagation ninja, I recommend succinct study materials from <a href="http://vision.stanford.edu/teaching/cs231n/syllabus.html">Stanford CS231n</a> <a href="http://cs231n.github.io/optimization-2/">here</a> and <a href="http://cs231n.github.io/neural-networks-case-study/">here</a>.</p>

<p>The materials will give you intuitive understanding about how to compute gradient with a backpropagation technique. This technique uses a chain rule and enjoys considering only the output and its inputs of single neurons, avoiding analyzing gradient of a single monolithic function in one shot.</p>

<p>The exercise <a href="http://vision.stanford.edu/teaching/cs231n/winter1516_assignment2.zip">FullyConnectedNets.ipynb</a> provided with the materials will introduce you to a modular layer design, and then use those layers to implement fully-connected networks of arbitrary depth.</p>

<blockquote>
  <p>â€¦ For each layer we will implement a <code>forward</code> and a <code>backward</code> function. The <code>forward</code> function will receive inputs, weights, and other parameters and will return both an output and a <code>cache</code> object storing data needed for the backward pass, like this:</p>
</blockquote>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">layer_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; Receive inputs x and weights w &quot;&quot;&quot;</span>
  <span class="c"># Do some computations ...</span>
  <span class="n">z</span> <span class="o">=</span> <span class="c"># ... some intermediate value</span>
  <span class="c"># Do some more computations ...</span>
  <span class="n">out</span> <span class="o">=</span> <span class="c"># the output</span>
   
  <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span> <span class="c"># Values we need to compute gradients</span>
   
  <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span></code></pre></div>

<blockquote>
  <p>The backward pass will receive upstream derivatives and the <code>cache</code> object, and will return gradients with respect to the inputs and weights, like this:</p>
</blockquote>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">layer_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Receive derivative of loss with respect to outputs and cache,</span>
<span class="sd">  and compute derivative with respect to inputs.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c"># Unpack cache values</span>
  <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">cache</span>
  
  <span class="c"># Use values in cache to compute derivatives</span>
  <span class="n">dx</span> <span class="o">=</span> <span class="c"># Derivative of loss with respect to x</span>
  <span class="n">dw</span> <span class="o">=</span> <span class="c"># Derivative of loss with respect to w</span>
  
  <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span></code></pre></div>

<blockquote>
  <p>After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.</p>
</blockquote>

<p>Below is a sample implementation of <code>layer_forward</code> and <code>layer_backward</code>.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">layer_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Computes the forward pass for an affine (fully-connected) layer.</span>
<span class="sd">  The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N</span>
<span class="sd">  examples, where each example x[i] has shape (d_1, ..., d_k). We will</span>
<span class="sd">  reshape each input into a vector of dimension D = d_1 * ... * d_k, and</span>
<span class="sd">  then transform it to an output vector of dimension M.</span>
<span class="sd">  Inputs:</span>
<span class="sd">  - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)</span>
<span class="sd">  - w: A numpy array of weights, of shape (D, M)</span>
<span class="sd">  - b: A numpy array of biases, of shape (M,)</span>
<span class="sd">  </span>
<span class="sd">  Returns a tuple of:</span>
<span class="sd">  - out: output, of shape (N, M)</span>
<span class="sd">  - cache: (x, w, b)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">out</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="c">#############################################################################</span>
  <span class="c"># TODO: Implement the affine forward pass. Store the result in out. You     #</span>
  <span class="c"># will need to reshape the input into rows.                                 #</span>
  <span class="c">#############################################################################</span>
  <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
  <span class="c">#############################################################################</span>
  <span class="c">#                             END OF YOUR CODE                              #</span>
  <span class="c">#############################################################################</span>
  <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span>

<span class="k">def</span> <span class="nf">layer_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Computes the backward pass for an affine layer.</span>

<span class="sd">  Inputs:</span>
<span class="sd">  - dout: Upstream derivative, of shape (N, M)</span>
<span class="sd">  - cache: Tuple of:</span>
<span class="sd">    - x: Input data, of shape (N, d_1, ... d_k)</span>
<span class="sd">    - w: Weights, of shape (D, M)</span>

<span class="sd">  Returns a tuple of:</span>
<span class="sd">  - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)</span>
<span class="sd">  - dw: Gradient with respect to w, of shape (D, M)</span>
<span class="sd">  - db: Gradient with respect to b, of shape (M,)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">cache</span>
  <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
  <span class="c">#############################################################################</span>
  <span class="c"># TODO: Implement the affine backward pass.                                 #</span>
  <span class="c">#############################################################################</span>
  <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">dw</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>
  <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="c">#############################################################################</span>
  <span class="c">#                             END OF YOUR CODE                              #</span>
  <span class="c">#############################################################################</span>
  <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span></code></pre></div>

<p>It is not difficult to derive gradients <code>dx</code> and <code>dw</code> and implement them in Python. The gradient <code>db</code>, however, requires careful thought. The expression <code>db = np.sum(dout, axis=0)</code> is not easy to understand. </p>

<p>First, you need to understand what really happens inside this simple equation <script type="math/tex">out = xw + b</script>. One way to understand this is by analyzing dimensions of the matrices. That is, <script type="math/tex">\underset{(N \times M)}{out} = \underset{(N \times D)}x\underset{(D \times M)}w + \underset{(1 \times M)}b</script>.</p>

<p>Do you notice incompatibility of <script type="math/tex">b</script>â€™s dimension and <script type="math/tex">out</script>â€™s? Think of it for a second and read on! </p>

<p>What happens in the expression is broadcast of <script type="math/tex">b</script> from <script type="math/tex">1 \times M</script> to <script type="math/tex">N \times M</script>. Intuitively, there is only one set of bias of size <script type="math/tex">M</script> which shares across all examples <script type="math/tex">x</script>. In fact, there is an <em>invisible input</em> for the bias term. That is, <script type="math/tex">\underset{(N \times M)}{out} = \underset{(N \times D)}x\underset{(D \times M)}w + \underset{(N \times 1)}{ix}\underset{(1 \times M)}b</script> where <script type="math/tex">ix</script> contains all 1â€™s and stays there just for the sake of facilitating later calculation. (As the term suggests, bias does not depend on the input.)</p>

<p>Thatâ€™s it for <code>layer_forward</code>. Now, letâ€™s proceed to <code>layer_backward</code>.</p>

<p>Considering <script type="math/tex">\underset{(N \times M)}{out} = \underset{(N \times D)}x\underset{(D \times M)}w + \underset{(N \times 1)}{ix}\underset{(1 \times M)}b</script>, we can derive <script type="math/tex">db</script> (in short, for <script type="math/tex">\frac{dLoss}{db}</script>) using a chain rule with <script type="math/tex">\underset{(1 \times M)}{db} = \underset{(1 \times N)}{ix^\mathsf{T}} \underset{(N \times M)}{dout}</script>. Recall that <script type="math/tex">ix</script> is all 1â€™s; therefore, the expression just sums <script type="math/tex">dout</script> along axis = 0, that is, <code>db = np.sum(dout, axis=0)</code>.</p>

<p>Not really trivial, is it?</p>

<h6 id="lessons-learned">Lessons Learned</h6>
<ul>
  <li>Understanding NumPyâ€™s broadcast is a must.</li>
  <li>Analyzing dimensions of matrices might come in handy.</li>
</ul>

<h6 id="further-reading">Further Reading</h6>
<ul>
  <li><a href="http://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc">Array Broadcasting in NumPy</a></li>
  <li><a href="http://cs231n.github.io/linear-classify/">http://cs231n.github.io/linear-classify/</a></li>
  <li><a href="http://cs231n.github.io/optimization-2/">http://cs231n.github.io/optimization-2/</a></li>
  <li><a href="http://cs231n.github.io/neural-networks-case-study/">http://cs231n.github.io/neural-networks-case-study/</a></li>
</ul>

<script type="text/javascript" async="" src="//cdn.mathjax.org/mathjax/latest/MathJax.js">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] },
    menuSettings: { zoom: "Click" }
  });
</script>


        </section>
        <footer class="post-footer">
          <section class="share">
            
              
                <a class="icon-twitter" href="http://twitter.com/share?text=A+Modular+Approach+to+Implementing+Fully-Connected+Neural+Networks&amp;url=https://mlxai.github.io/2017/01/10/a-modular-approach-to-implementing-fully-connected-neural-networks"
                  onclick="window.open(this.href, 'twitter-share', 'width=550,height=255');return false;">
                <i class="fa fa-twitter"></i><span class="hidden">twitter</span>
                </a>
              
            
              
            
          </section>
        </footer>
        <div class="bottom-teaser cf">
          <div class="isLeft">
            <h5 class="index-headline featured"><span>Written by</span></h5>
            <section class="author">
              <div class="author-image" style="background-image: url(/assets/images/nimit-opt.jpg)">Blog Logo</div>
              <h4>Nimit Pattanasri</h4>
              <p class="bio"></p>
              <hr>
              <p class="published">Published <time datetime="2017-01-10 00:00">10 Jan 2017</time></p>
            </section>
          </div>
          
          <div class="isRight">
            <h5 class="index-headline featured"><span>Supported by</span></h5>
            <footer class="site-footer">
              <section class="poweredby">Proudly published with <a href="http://jekyllrb.com"> Jekyll</a></section>              
              <div class="inner">
                <section class="copyright">All content copyright <a href="/">Nimit Pattanasri</a> &copy; 2017<br>All rights reserved. <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> </span></a></section>
              </div>
            </footer>
          </div>
        </div>
        
<div id="disqus_thread"></div>
<script>
var disqus_config = function () {
this.page.url = "https://mlxai.github.io/2017/01/10/a-modular-approach-to-implementing-fully-connected-neural-networks.html"; // <--- use canonical URL
this.page.identifier = "/2017/01/10/a-modular-approach-to-implementing-fully-connected-neural-networks";
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//mlxai.disqus.com/embed.js'; // <--- use Disqus shortname

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

      </article>
    </main>
    <div class="bottom-closer">
      <div class="background-closer-image"  style="background-image: url(/assets/images/cover-opt.jpg)">
        Image
      </div>
      <div class="inner">
        <h1 class="blog-title">ML x AI</h1>
        <h2 class="blog-description">Notes for my journey in machine learning and artificial intelligence
</h2>
        <a href=/ class="btn">Back to Overview</a>
      </div>
    </div>
    <script src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
<script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/assets/js/index.js"></script>
<script type="text/javascript" src="/assets/js/readingTime.min.js"></script>
<script>
(function ($) {
  "use strict";
  $(document).ready(function(){

    var $window = $(window),
    $image = $('.post-image-image, .teaserimage-image');
    
      $window.on('scroll', function() {
        var top = $window.scrollTop();

        if (top < 0 || top > 1500) { return; }
        $image
          .css('transform', 'translate3d(0px, '+top/3+'px, 0px)')
          .css('opacity', 1-Math.max(top/700, 0));
      });
      $window.trigger('scroll');

      var height = $('.article-image').height();
      $('.post-content').css('padding-top', height + 'px');

      $('a[href*=#]:not([href=#])').click(function() {
        if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'')
         && location.hostname == this.hostname) {
          var target = $(this.hash);
          target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
          if (target.length) {
            $('html,body').animate({ scrollTop: target.offset().top }, 500);
            return false;
          }
        }
      });

  });
}(jQuery));
</script>


  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-6345302-31', 'auto');
  ga('send', 'pageview');

</script>


<!--
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<script>
  $("script[type='math/tex']").replaceWith(function() {
      var tex = $(this).text();
      return katex.renderToString(tex, {displayMode: false});
  });

  $("script[type='math/tex; mode=display']").replaceWith(function() {
      var tex = $(this).html();
      return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
  });
</script>
-->

  </body>
</html>
